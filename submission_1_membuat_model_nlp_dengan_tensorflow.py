# -*- coding: utf-8 -*-
"""Membuat_Model_NLP_dengan_TensorFlow.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11rEQkGvS5zKtdDh_4oiAYK7G2UMPU8gy

## Import Library
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
# %matplotlib inline
import seaborn as sns
from google.colab import files

"""## Download & Load dataset
### Datasets downloaded from kaggle https://www.kaggle.com/hgultekin/bbcnewsarchive


"""

uploaded = files.upload()

df = pd.read_csv('bbc-news-data.csv', sep='\t')

print(f'Jumlah datasets: {len(df)}')
df

# delete columns (unused column)
df = df.drop(columns=['filename'])
df

from matplotlib import pyplot as plt
import seaborn as sns
ax = df.groupby('category').size().plot(kind='barh', color=sns.palettes.mpl_palette('Dark2'))
plt.gca().spines[['top', 'right',]].set_visible(False)
ax.bar_label(ax.containers[0], fontsize=10)

# data info
df.info()

"""## Data Cleansing


*   Lower-case all characters
*   Removing punctuation
*   Removing number
*   Remove stopwords
*   Do lemmatization







"""

import re
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer

nltk.download('stopwords')
nltk.download('wordnet')

stopword = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()

df_new = df.copy()

# lower-case all characters
df_new.title = df_new.title.apply(lambda x: x.lower())
df_new.content = df_new.content.apply(lambda x: x.lower())

# removing punctuation
def cleaner(data):
    return(data.translate(str.maketrans('','', string.punctuation)))
    df_new.title = df_new.title.apply(lambda x: cleaner(x))
    df_new.content = df_new.content.apply(lambda x: lem(x))

## lematization
lemmatizer = WordNetLemmatizer()

def lem(data):
    pos_dict = {'N': wn.NOUN, 'V': wn.VERB, 'J': wn.ADJ, 'R': wn.ADV}
    return(' '.join([lemmatizer.lemmatize(w,pos_dict.get(t, wn.NOUN)) for w,t in nltk.pos_tag(data.split())]))
    df_new.title = df_new.title.apply(lambda x: lem(x))
    df_new.content = df_new.content.apply(lambda x: lem(x))

# removing number
def rem_numbers(data):
    return re.sub('[0-9]+','',data)
    df_new['title'].apply(rem_numbers)
    df_new['content'].apply(rem_numbers)

# removing stopword
st_words = stopwords.words()
def stopword(data):
    return(' '.join([w for w in data.split() if w not in st_words ]))
    df_new.title = df_new.title.apply(lambda x: stopword(x))
    df_new.content = df_new.content.apply(lambda x: lem(x))

df_new.head()

"""## Encoding The Category"""

# data category one-hot-encoding
category = pd.get_dummies(df_new.category)
df_new_cat = pd.concat([df_new, category], axis=1)
df_new_cat = df_new_cat.drop(columns='category')
df_new_cat

"""## Splitting Data"""

# change dataframe value to numpy array
X = df_new_cat['title'].values + '' + df_new_cat['content'].values
y = df_new_cat[['business',
                'entertainment',
                'politics',
                'sport',
                'tech']].values

# Split data into training and validation (80% : 20%)

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y,
                                                    random_state= 42,
                                                    test_size=0.2,
                                                    shuffle=True)

print("Total data train: ", len(y_train))
print("Total data test: ", len(y_test))

"""## Tokenizer"""

# Tokenizer

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

tokenizer = Tokenizer(num_words=5000, oov_token='x', filters='!"#$%&()*+,-./:;<=>@[\]^_`{|}~ ')
tokenizer.fit_on_texts(X_train)
tokenizer.fit_on_texts(X_test)

word_index = tokenizer.word_index

X_train_seq = tokenizer.texts_to_sequences(X_train)
X_test_seq = tokenizer.texts_to_sequences(X_test)

padded_train = pad_sequences(X_train_seq)
padded_test = pad_sequences(X_test_seq)


print(f'length of words: {len(word_index)}')

"""## Modelling"""

# callback function
class myCallback(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if(logs.get('accuracy')>0.9 and logs.get('val_accuracy')>0.9):
      self.model.stop_training = True
      print("\nThe accuracy of the training set and the validation set has reached > 90%!")
callbacks = myCallback()

# model
model = tf.keras.Sequential([
    tf.keras.layers.Embedding(input_dim=5000, output_dim=64),
    tf.keras.layers.LSTM(128),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.Dense(5, activation='softmax')
])

model.compile(optimizer='adam', metrics=['accuracy'], loss='categorical_crossentropy',)

model.summary()

num_epochs = 50
history = model.fit(padded_train, y_train, epochs=num_epochs,
                    validation_data=(padded_test, y_test),
                    callbacks=[callbacks], verbose=2, validation_steps=30)

"""## Plotting"""

# plot of accuracy
import matplotlib.pyplot as plt
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Model Accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

# plot of loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model Loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

"""## Classification Report"""

from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import numpy as np

train_prediction = np.argmax(model.predict(padded_train), axis=1)
test_prediction = np.argmax(model.predict(padded_test), axis=1)

print("Accuracy data train : ", accuracy_score(train_prediction,
                                               np.argmax(y_train, axis=1)))
print("Accuracy data test : ", accuracy_score(test_prediction,
                                              np.argmax(y_test, axis=1)))

print()

print("Classification Report")
print(classification_report(test_prediction, np.argmax(y_test, axis=1), target_names=['Business', 'Entertaintment', 'Politics', 'Sport', 'Tech']))

print("Confussion Matrix")
print(confusion_matrix(test_prediction, np.argmax(y_test, axis=1)))